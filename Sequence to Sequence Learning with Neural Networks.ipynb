{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Sequence-to-Sequence Learning with Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [What is a sequence to sequence network ?](#sec1)\n",
    "2. [How is it made ?](#sec2)\n",
    "    1. [Principles](#sec2-A)\n",
    "    2. [Why do we use RNNs ?](#sec2-B)\n",
    "    3. [LSTM and GRU](#sec2-C)\n",
    "    4. [Recap](#sec2-D)\n",
    "    5. [Attention](#sec2-E)\n",
    "3. [Let's do a French to English translator](#sec3)\n",
    "    1. [Preprocess the data](#sec3-1)\n",
    "    2. [The model](#sec3-2)\n",
    "    3. [Functions to Vectorize the training data](#sec3-3)\n",
    "    4. [Training the model](#sec3-4)\n",
    "    5. [Evaluation](#sec3-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytorch library is required for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at the sequence-to-sequence model. Through its architecture, we will see its principles, these different neural network cells, and the principle of attention. A practical application will conclude this notebook by a French to English sequence-to-sequence translator, adapted from the pyTorch tutorial \"NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a>What is a Sequence-to-Sequence model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Sequence-to-Sequence</i> (abrv. Seq2Seq) models are deep learning models that take a sequence of items (sentences, medical signals, speech waveforms, time-series, …) and output another sequence of items, hence its name \"sequence to sequence\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_1.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is explained in the pioneering paper : [Sutskever et al., 2014](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) (pretty new !)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to sequence models have proven their effectiveness for many tasks and can be used in machine translation, articles and books summarization, image captioning, speech recognition, waveforms/time-series forecasting, predicting stock market trends, handwriting generation and many more ! \n",
    "\n",
    "In the case of Neural Machine Translation, the input is a series of words, and the output is the translated series of words. Until 2014, [Statistical Machine Translation](https://en.wikipedia.org/wiki/Statistical_machine_translation) was by far the most widely used machine translation method, using statistical models. The introduction of [Neural Machine Translation](https://en.wikipedia.org/wiki/Neural_machine_translation) has significantly increased performance, and for instance Google introduced in November 2016 its new [Google Neural Machine Translation](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation) for Google Translate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_mt.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id=\"sec2\"></a>How is it made ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. <a id=\"sec2-A\"></a>Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seq2seq network is generally made of two neural networks. The first one is an **encoder**, which encodes an input sequence to a fixed-length context vector (you can think of the context vector as being an abstract representation of the entire input sentence, we will talk more about his context vector later). The second one is a **decoder** which receives this context vector and learns to output the target (output) sequence by generating it one item at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_2.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input is **3** circles, and the output is **4** triangles. \n",
    "\n",
    "Indeed, for tranlation for example, the length of the input sequence in language A is not necessarily equal to the length of the output sequence in language B. \"Je suis étudiant\" (3 words) becomes \"I am a student\" (4 words). A seq2seq model is able to take a variable-length sequence as an input, and return a variable-length sequence as an output, using a fixed-sized model by encoding many inputs into one vector, and decoding from one vector into many outputs.\n",
    "\n",
    "In order to two that, the encoder and decoder neural networks are (commonly) **RNNs (Recurrent Neural Networks)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-B\"></a>B. Why RNNs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Long-term dependencies\n",
    "\n",
    "Remember, as we have seen with Denis with the Time-series Forecasting notebook, recurrent neural networks depend on the previous state for the current state's computation. \n",
    "\n",
    "Instead of simply prediction $Y = f(x)$ as in feed-forward neural networks, recurrent networks do $Y_1 = f(x_1, f(x_0))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/unrolled_rnn.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that at every point in time, it takes as input its own previous state and the new input at that time step.\n",
    "<div class=\"alert alert-success\">\n",
    "RNNs keep information about their previous state.\n",
    "</div>\n",
    "\n",
    "Each state is a function of the previous state, which is the function of its previous state, and so on. So, state n contain information from all past timesteps. And we need this to predict sequences. Indeed, elements in the sequences and the order of these elements have a strong relationship. For machine translation, a sentence contains words in a certain order and some of which have a strong influence on others. \n",
    "\n",
    "For example, let's translate using Google Translate \"Je surveille mes actions tous les jours\" in english :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/mt_ex_1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's translate \"A la bourse, je surveille mes actions tous les jours\" :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/mt_ex_2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how \"actions\" becomes \"stocks\" thanks to the memory of the RNN when we add the stock market context at the beginning of the sentence.\n",
    "\n",
    "##### - Variable-length sequence\n",
    "\n",
    "The two RNNS working together frees us from sequence length which makes it ideal for translation between two languages and opens a whole new range of problems which can be solved using such architecture. How a sequence starts and ends ? Example with a german to english translator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/sos-eos.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes as input a sequence starting with the SOS (Start Of Sequence) token and ending with the EOS (End Of Sequence) token. As long as the decoder doesn't predict the EOS token, the size of the output sequence can increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-C\"></a>C. LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've talked about RNNs that keep track of temporal dependencies. In reality, simple RNNs can have difficulty handling long-term dependencies. Imagine a sequence problem that involves predicting a word in a sentence. Example : \"In France, I had a great time and I learnt some of the ______ language.\" The information of the word \"France\" can be found in hidden states but often is already too abstract. \n",
    "\n",
    "Moreover, when learning, in some cases the gradient can be very small and can prevent the network weights from changing value. Traditional activation functions such as the hyperbolic tangent function have gradients in the range (-1, 1). Thus, the backpropagation trough time calculates gradients by multiplying very small numbers. And the more we calculate the errors due to further and further back time steps, the smaller and smaller the gradients will be, because of the product of small numbers. Parameters of the simple RNN become biased to capture short-term dependencies. This is called the [Vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).\n",
    "\n",
    "A seq2seq model could use more complex recurrent neural networks without this problem, such as **LSTM (Long Short-Term Memory)** or a **GRU (Gated Recurrent Unit)**. \n",
    "\n",
    "##### - LSTM\n",
    "As we saw with Denis with the Time-series Forecasting notebook, a LSTM is a more complex RNN explicitly designed to avoid the long-term dependency problem. Instead of just taking in a hidden state and returning a new hidden state per time-step, a LSTM cell take in and return a separate cell state, $c_t$, per time-step.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    " $h_t = \\text{simpleRNN}(x_t, h_{t-1})\\\\\n",
    " (h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))$\n",
    "</div>\n",
    "Example of a LSTM cell :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/lstm-cell.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i)\\\\\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} +b_f)\\\\\n",
    "\\tilde{C}_t = \\tanh(W_{x\\tilde{C}}x_t + W_{h\\tilde{C}}h_{t-1} + b_\\tilde{C})\\\\\n",
    "C_t = f_tc_{t-1}+i_t\\tilde{C}_t\\\\\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1}+b_o)\\\\\n",
    "h_t = o_t\\tanh(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to LSTMs is the cell state, which, in theory, can carry relevant information trhoughout the entire processing of the sequence. The horizontal line running through the cell is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/lstm-conveyor-belt.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM, using gates, have the ability to remove or add information to the cell state. Gates are a way to optionally let information through and they can be trained to keep information from long ago. It can keep information in the cell state, which might not be relevant at this time step, but might be relevant at a much later time step. Differents types of LSTM cells exist, if you want to learn more about LSTM, you can read [this blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "<div class=\"alert alert-success\">LSTM cells are able to keep track of information throughout many timesteps.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - GRU\n",
    "A GRU, Gated Recurrent Unit, get rid of the cell state, has only two gates (a reset gate and an update gate) and used only the hidden state to transfer information. GRUs are a little speedier to train than LSTMs. Example of a GRU :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/gru.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z_t = \\sigma(W_{xz}x_t + W_{hz}h_{t-1} +b_z)\\\\\n",
    "r_t = \\sigma(W_{xr}x_t + W_{hr}h_{t-1} +b_r)\\\\\n",
    "h_t = z_t h_{t-1} + (1-z_t)\\tanh(W_{xh}x_t + W_{hh}h_{t-1}r_t + b_h)\\\\\n",
    "$$\n",
    "\n",
    "As with LSTMs, there are possible variants of GRU. If you are interested, GRU is the subject of one of AML's notebooks this year (topic number 31), so you can learn more about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-D\"></a>D. Recap\n",
    "\n",
    "Here's a animation showing what we have for a seq2seq machine translation model for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_4.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN cells can be simple RNN or LSTM cells or GRU cells.\n",
    "\n",
    "In theory, the context vector (the final hidden state of the encoder) will contain semantic information about the query sentence that is input to the bot. \n",
    "**Problem**: The context vector is responsible for representing the entire input sequence. The output sequence relies heavily on this vector, making it challenging for the model to deal with long sentences. A solution was proposed in the papers [Bahdanau et al., 2014](https://arxiv.org/pdf/1409.0473.pdf) and [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf) : **attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-E\"></a>E. Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention allows the model to focus on the relevant parts of the input sequence at every stage of the output sequence allowing the context to be preserved from beginning to end. Instead of sending only one single hidden state vector to the decoder, we send an attention vector created from a linear combination of all previous hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_5.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every step the decoder can select a different part of the input sentence to consider. So, every step, a new attention vector (so a new linear combination) is calculated to be relevant for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_6.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the translation of the sentence \"L'accord sur la zone économique européènne a été signé en août 1992\", we can visualize the attention matrix giving different weights to the hidden states depending on the time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"480\" height=\"360\"  src=\"Images/attention.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the model, for unambiguous words like \"août\" or \"1992\", gives little importance to the other words in the sentence. You can also see that for \"zone économique européenne\", the model adapts to the reversed order between French and English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a nutshell**, we have seen that a seq2seq model is composed of two RNNs working together, one as an encoder and one as a decoder. These RNNs can be simple, or more complex using LSTM or GRU cells. For the context vector passing from the encoder to the decoder, it consists of all the hidden states of the encoder. For each step, an attention vector is computed to give different importance to the different hidden states, so that the decoder can focus on the relevant parts for the time step under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id=\"sec3\"></a>Let's do a French to English translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import re, unicodedata\n",
    "import random\n",
    "\n",
    "#setting up device for use with pyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device used is \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id=\"sec3-1\"></a>Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/enfratexts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0  # Start-of-sentence token\n",
    "EOS_token = 1  # End-of-sentence token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to vectorize the data. Each unique word we encounter in our dataset will be assigned to a unique index. \n",
    "\n",
    "We will first define languages containing all the words in our dataset for the two different languages (similar to what we did in Natural Language Processing notebooks). For this, we use the Lang class. \n",
    "\n",
    "**Lang class**\n",
    "<ul>\n",
    "<li> maps words to indexes (word2index)\n",
    "<li> keeps the number of occurences of each word (word2count)\n",
    "<li> maps indexes to words (index2word)\n",
    "<li> keeps a total word count of words (n_words)\n",
    "<li> provides a method for adding a word to the language (addWord)\n",
    "<li> provides a method for adding all words in a sentence (addSentence)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to create our languages, we need to build french/english sentence pairs. We must also convert the Unicode strings to ASCII using unicodeToAscii, convert all letters to lowercase and trim all non-letter characters except for basic punctuation (normalizeString). Finally, to aid in training convergence, we will filter out sentences with length greater than the MAX_LENGTH threshold (filterPairs).\n",
    "\n",
    "For the preprocessing, we are not going to apply Lemmatization or Stemming (cf. NLP courses) because we want to keep the data readable by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to train something quickly, we’re filtering to sentences that begin with the form \"I am\" or \"He is\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"%s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(\" -\", input_lang.name, input_lang.n_words, \"words\")\n",
    "    print(\" -\", output_lang.name, output_lang.n_words, \"words\")\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(\"Example of a pair : \", random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 10,599 pairs of sentences, one in French and another one being its English translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3-2\"></a>3.2. The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use GRU cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/encoder-mt.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "Write the embedding layer and GRU layer code using torch.nn. Think about the dimensions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding Layer\n",
    "        #TODO\n",
    "        \n",
    "        # GRU Layer\n",
    "        #TODO\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Embedding the input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load corrections/EncoderRNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer  is used to represent words by dense vectors where a vector represents the projection of the word into a continuous vector space. Each word in the vocabulary corresponds to a single vector of real numbers of fixed dimension. As we have seen in NLP, this is an improvement over the traditional bag-of-word model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/decoder-mt.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "    \n",
    "Write the embedding layer, the GRU layer and the out layer code using torch.nn. The out layer is a linear layer mapping the ouput of the GRU to the output size.\n",
    "    \n",
    "Using the decoder's function and the blueprint, write the forward function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #Embedding Layer\n",
    "        #TODO\n",
    "        \n",
    "        #GRU layer\n",
    "        #TODO\n",
    "        \n",
    "        #Out layer\n",
    "        #TODO\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Embedding the input and applying relu (use F.relu)\n",
    "        #TODO\n",
    "        \n",
    "        #GRU and softmax\n",
    "        #TODO\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load corrections/DecoderRNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/attention-mt.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"BMM\" performs a batch matrix-matrix product of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice</b><br>\n",
    "    \n",
    "Write the embedding layer, the linear attn layer, the linear attn_combine layer, and the GRU layer code using torch.nn.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p #dropout proba\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.embedding = #TODO\n",
    "        \n",
    "        #Linear layer\n",
    "        self.attn = #TODO\n",
    "        \n",
    "        #Linear Layer\n",
    "        self.attn_combine = #TODO\n",
    "        \n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        #GRU layer\n",
    "        self.gru = #TODO\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #First, the input (target sequence) is embedded. Some weights are randomly zeroed out to facilitate\n",
    "        #learning with the attention mechanism\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        #Attention is computed by combining the context vectors and the embedded input\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        #Attention is applied on the encoded original sentence (in english)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        #We retrieve the embedded input and the context vector (with attention applied), and combine the two tensors\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #The attended part of the input is fed into the lstm, conditioned by the hidden and cell states\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #Retrieve token probabilities\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        #In addition to the output and hidden states that are necessary for iterating, we return the attention\n",
    "        #weights, that will provide some form of explainability\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load corrections/AttnDecoderRNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know how to implement a seq2seq model. If you want to reuse this code, you may not need the embedding layers that are useful here because we have words as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3-3\"></a>3.3. Functions to Vectorize the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, for each pair we will need an input tensor (index of words in the input sentence) and a target tensor (index of words in the target sentence). When creating these vectors, we will add the EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3-4\"></a>3.4. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. One step training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    #We pass each input token to the encoder. At each step, we retrive the output and the hidden/cell states,\n",
    "    #forming the context vector. The context vector is fed back to the encoder for the next step.\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device) #Decoder intilized with a SOS token\n",
    "    decoder_hidden = encoder_hidden #Passing the context vector from the encoder to the decoder\n",
    "\n",
    "    # use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    \n",
    "    #Backward propagation\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3. Multiple training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    #Setup optimizers\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        \n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        # Run train function\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        #Print\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"iter = {0} / {1} = {2:.1f}%, loss avg = {3:.2f}\".format(iter, n_iters, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        #Plot\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU, we can see what the training of the model looks like. Else, move on to the next cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 100 / 10000 = 1.0%, loss avg = 4.49\n",
      "iter = 200 / 10000 = 2.0%, loss avg = 3.76\n",
      "iter = 300 / 10000 = 3.0%, loss avg = 3.59\n",
      "iter = 400 / 10000 = 4.0%, loss avg = 3.44\n",
      "iter = 500 / 10000 = 5.0%, loss avg = 3.64\n",
      "iter = 600 / 10000 = 6.0%, loss avg = 3.33\n",
      "iter = 700 / 10000 = 7.0%, loss avg = 3.49\n",
      "iter = 800 / 10000 = 8.0%, loss avg = 3.43\n",
      "iter = 900 / 10000 = 9.0%, loss avg = 3.22\n",
      "iter = 1000 / 10000 = 10.0%, loss avg = 3.11\n",
      "iter = 1100 / 10000 = 11.0%, loss avg = 3.18\n",
      "iter = 1200 / 10000 = 12.0%, loss avg = 3.14\n",
      "iter = 1300 / 10000 = 13.0%, loss avg = 3.01\n",
      "iter = 1400 / 10000 = 14.0%, loss avg = 3.02\n",
      "iter = 1500 / 10000 = 15.0%, loss avg = 2.97\n",
      "iter = 1600 / 10000 = 16.0%, loss avg = 2.99\n",
      "iter = 1700 / 10000 = 17.0%, loss avg = 2.88\n",
      "iter = 1800 / 10000 = 18.0%, loss avg = 3.01\n",
      "iter = 1900 / 10000 = 19.0%, loss avg = 2.81\n",
      "iter = 2000 / 10000 = 20.0%, loss avg = 3.16\n",
      "iter = 2100 / 10000 = 21.0%, loss avg = 2.76\n",
      "iter = 2200 / 10000 = 22.0%, loss avg = 2.81\n",
      "iter = 2300 / 10000 = 23.0%, loss avg = 2.84\n",
      "iter = 2400 / 10000 = 24.0%, loss avg = 2.77\n",
      "iter = 2500 / 10000 = 25.0%, loss avg = 2.62\n",
      "iter = 2600 / 10000 = 26.0%, loss avg = 2.76\n",
      "iter = 2700 / 10000 = 27.0%, loss avg = 2.98\n",
      "iter = 2800 / 10000 = 28.0%, loss avg = 2.68\n",
      "iter = 2900 / 10000 = 29.0%, loss avg = 2.66\n",
      "iter = 3000 / 10000 = 30.0%, loss avg = 2.81\n",
      "iter = 3100 / 10000 = 31.0%, loss avg = 2.84\n",
      "iter = 3200 / 10000 = 32.0%, loss avg = 2.73\n",
      "iter = 3300 / 10000 = 33.0%, loss avg = 2.78\n",
      "iter = 3400 / 10000 = 34.0%, loss avg = 2.66\n",
      "iter = 3500 / 10000 = 35.0%, loss avg = 2.68\n",
      "iter = 3600 / 10000 = 36.0%, loss avg = 2.58\n",
      "iter = 3700 / 10000 = 37.0%, loss avg = 2.72\n",
      "iter = 3800 / 10000 = 38.0%, loss avg = 2.77\n",
      "iter = 3900 / 10000 = 39.0%, loss avg = 2.68\n",
      "iter = 4000 / 10000 = 40.0%, loss avg = 2.69\n",
      "iter = 4100 / 10000 = 41.0%, loss avg = 2.75\n",
      "iter = 4200 / 10000 = 42.0%, loss avg = 2.78\n",
      "iter = 4300 / 10000 = 43.0%, loss avg = 2.68\n",
      "iter = 4400 / 10000 = 44.0%, loss avg = 2.66\n",
      "iter = 4500 / 10000 = 45.0%, loss avg = 2.42\n",
      "iter = 4600 / 10000 = 46.0%, loss avg = 2.49\n",
      "iter = 4700 / 10000 = 47.0%, loss avg = 2.65\n",
      "iter = 4800 / 10000 = 48.0%, loss avg = 2.65\n",
      "iter = 4900 / 10000 = 49.0%, loss avg = 2.41\n",
      "iter = 5000 / 10000 = 50.0%, loss avg = 2.49\n",
      "iter = 5100 / 10000 = 51.0%, loss avg = 2.57\n",
      "iter = 5200 / 10000 = 52.0%, loss avg = 2.61\n",
      "iter = 5300 / 10000 = 53.0%, loss avg = 2.55\n",
      "iter = 5400 / 10000 = 54.0%, loss avg = 2.52\n",
      "iter = 5500 / 10000 = 55.0%, loss avg = 2.52\n",
      "iter = 5600 / 10000 = 56.0%, loss avg = 2.56\n",
      "iter = 5700 / 10000 = 57.0%, loss avg = 2.67\n",
      "iter = 5800 / 10000 = 58.0%, loss avg = 2.59\n",
      "iter = 5900 / 10000 = 59.0%, loss avg = 2.46\n",
      "iter = 6000 / 10000 = 60.0%, loss avg = 2.51\n",
      "iter = 6100 / 10000 = 61.0%, loss avg = 2.60\n",
      "iter = 6200 / 10000 = 62.0%, loss avg = 2.50\n",
      "iter = 6300 / 10000 = 63.0%, loss avg = 2.58\n",
      "iter = 6400 / 10000 = 64.0%, loss avg = 2.42\n",
      "iter = 6500 / 10000 = 65.0%, loss avg = 2.50\n",
      "iter = 6600 / 10000 = 66.0%, loss avg = 2.48\n",
      "iter = 6700 / 10000 = 67.0%, loss avg = 2.45\n",
      "iter = 6800 / 10000 = 68.0%, loss avg = 2.48\n",
      "iter = 6900 / 10000 = 69.0%, loss avg = 2.42\n",
      "iter = 7000 / 10000 = 70.0%, loss avg = 2.56\n",
      "iter = 7100 / 10000 = 71.0%, loss avg = 2.40\n",
      "iter = 7200 / 10000 = 72.0%, loss avg = 2.51\n",
      "iter = 7300 / 10000 = 73.0%, loss avg = 2.56\n",
      "iter = 7400 / 10000 = 74.0%, loss avg = 2.44\n",
      "iter = 7500 / 10000 = 75.0%, loss avg = 2.30\n",
      "iter = 7600 / 10000 = 76.0%, loss avg = 2.43\n",
      "iter = 7700 / 10000 = 77.0%, loss avg = 2.42\n",
      "iter = 7800 / 10000 = 78.0%, loss avg = 2.30\n",
      "iter = 7900 / 10000 = 79.0%, loss avg = 2.34\n",
      "iter = 8000 / 10000 = 80.0%, loss avg = 2.11\n",
      "iter = 8100 / 10000 = 81.0%, loss avg = 2.33\n",
      "iter = 8200 / 10000 = 82.0%, loss avg = 2.33\n",
      "iter = 8300 / 10000 = 83.0%, loss avg = 2.41\n",
      "iter = 8400 / 10000 = 84.0%, loss avg = 2.25\n",
      "iter = 8500 / 10000 = 85.0%, loss avg = 2.24\n",
      "iter = 8600 / 10000 = 86.0%, loss avg = 2.38\n",
      "iter = 8700 / 10000 = 87.0%, loss avg = 2.27\n",
      "iter = 8800 / 10000 = 88.0%, loss avg = 2.27\n",
      "iter = 8900 / 10000 = 89.0%, loss avg = 2.49\n",
      "iter = 9000 / 10000 = 90.0%, loss avg = 2.28\n",
      "iter = 9100 / 10000 = 91.0%, loss avg = 2.28\n",
      "iter = 9200 / 10000 = 92.0%, loss avg = 2.32\n",
      "iter = 9300 / 10000 = 93.0%, loss avg = 2.32\n",
      "iter = 9400 / 10000 = 94.0%, loss avg = 2.26\n",
      "iter = 9500 / 10000 = 95.0%, loss avg = 2.30\n",
      "iter = 9600 / 10000 = 96.0%, loss avg = 2.31\n",
      "iter = 9700 / 10000 = 97.0%, loss avg = 2.17\n",
      "iter = 9800 / 10000 = 98.0%, loss avg = 2.12\n",
      "iter = 9900 / 10000 = 99.0%, loss avg = 2.16\n",
      "iter = 10000 / 10000 = 100.0%, loss avg = 2.13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    hidden_size = 256\n",
    "\n",
    "    n_iter = 10000\n",
    "\n",
    "    encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "    attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "    trainIters(encoder, attn_decoder, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've trained a fairly high-performance model beforehand. We can load it directly and avoid waiting for the training which took me about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(encoder,'encoder')\n",
    "#torch.save(attn_decoder,'attn_decoder')\n",
    "\n",
    "encoder_2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder_2 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "encoder_2 =torch.load('encoder')\n",
    "attn_decoder_2 = torch.load('attn_decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3-5\"></a>3.5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \n",
    "    with torch.no_grad(): #Freeze gradient\n",
    "        \n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        \n",
    "        #Decoder\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # Initialize decoder input with the SOS token\n",
    "        decoder_hidden = encoder_hidden #Feed the encoder context vectors to the decoder\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder_2, attn_decoder_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder_2, attn_decoder_2, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "evaluateAndShowAttention(\"je ne suis pas une ivrogne .\")\n",
    "evaluateAndShowAttention(\"c est vous le doyen .\")\n",
    "evaluateAndShowAttention(\"je ne suis pas celle que vous pensez .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model is quite efficient in translation. However, the attention does not seem to be perfectly tuned. It gives importance to words that are not directly correlated to the step in question. To go further, you could try to use LSTM cells instead of GRU, you could also try to use longer sentences and not limit the model to sizes smaller than MAX_LENGTH. You could also try sentences that don't start with predefined prefixes like \"i am \", \"i m \", \"he is\", \"he s \", etc. You could also use techniques such as \"teacher forcing\" to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sources\"></a>Sources\n",
    "\n",
    "- Sequence to Sequence Learning with Neural Networks : [Sutskever et al., 2014](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "- Learning Phrase Representations using RNN Encoder–Decoderfor Statistical Machine Translation : [Cho et al., 2014](https://arxiv.org/pdf/1406.1078v3.pdf)\n",
    "- Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) : [Jay Alammar, 2018](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "- MIT 6.S191 (2018): Sequence Modeling with Neural Networks : [Harini Suresh's Lecture (Youtube video)](https://www.youtube.com/watch?v=CznICCPa63Q)\n",
    "- Neural machine translation by jointly learning to align and translate : [Bahdanau et al., 2014](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "- Effective Approaches to Attention-based Neural Machine Translation : [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf)\n",
    "- Attention is all you need : [Ashish Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- Attention - Seq2Seq Models : [Pranay Dugar, 2019](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)\n",
    "- Recurrent neural network : [Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "- Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText : [Ben Trevett (Github)](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "- Time-series Forecasting : [Denis Wilson's notebook](https://github.com/erachelson/MLclass/tree/master/13%20-%20Time-series%20Forecasting)\n",
    "- Natural Language Processing : [Gautier Durantin's notebooks](https://github.com/erachelson/MLclass/tree/master/12%20-%20Natural%20Language%20Processing)\n",
    "- Vanishing gradient problem : [Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "- The Vanishing Gradient Problem : [Chi-Feng Wang, 2019](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "- Long short-term memory : [Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "- Gated recurrent unit : [Wikipedia](https://en.wikipedia.org/wiki/Gated_recurrent_unit)\n",
    "- Illustrated Guide to LSTM’s and GRU’s: A step by step explanation : [Michael Nguyen, 2018](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "- Understanding LSTM Networks : [Christopher Olah, 2015](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Understanding GRU Networks : [Simeon Kostadinov, 2017](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)\n",
    "- Modern Recurrent Neural Networks : [Dive into Deep Learning course](https://d2l.ai/chapter_recurrent-modern/index.html)\n",
    "- NLP | Sequence to Sequence Networks| Part 1| Processing text data : [Mohammed Ma'amari, 2018](https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72)\n",
    "- NLP From Scratch: Translation with a Sequence to Sequence Network and Attention : [Pytorch Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
