{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Sequence-to-Sequence Learning with Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [What is a sequence to sequence network ?](#sec1)\n",
    "2. [How is it made ?](#sec2)\n",
    "    1. [Principles](#sec2-A)\n",
    "    2. [Why do we use RNNs ?](#sec2-B)\n",
    "    3. [LSTM and GRU](#sec2-C)\n",
    "    4. [Recap](#sec2-D)\n",
    "    5. [Attention](#sec2-E)\n",
    "3. [Code](#sec3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a>What is a Sequence-to-Sequence model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Sequence-to-Sequence</i> (abrv. Seq2Seq) models are deep learning models that take a sequence of items (sentences, medical signals, speech waveforms, time-series, …) and output another sequence of items, hence its name \"sequence to sequence\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_1.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is explained in the pioneering paper : [Sutskever et al., 2014](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) (pretty new !)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to sequence models have proven their effectiveness for many tasks and can be used in machine translation, articles and books summarization, image captioning, speech recognition, waveforms/time-series forecasting, predicting stock market trends, handwriting generation and many more ! \n",
    "\n",
    "In the case of Neural Machine Translation, the input is a series of words, and the output is the translated series of words. Until 2014, [Statistical Machine Translation](https://en.wikipedia.org/wiki/Statistical_machine_translation) was by far the most widely used machine translation method, using statistical models. The introduction of [Neural Machine Translation](https://en.wikipedia.org/wiki/Neural_machine_translation) has significantly increased performance, and for instance Google introduced in November 2016 its new [Google Neural Machine Translation](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation) for Google Translate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_mt.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id=\"sec2\"></a>How is it made ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. <a id=\"sec2-A\"></a>Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seq2seq network is generally made of two neural networks. The first one is a **encoder**, which encodes a variable length input sequence to a fixed-length context vector (you can think of the context vector as being an abstract representation of the entire input sentence, we will talk more about his context vector later). The second one is a **decoder** which receives this context vector and learns to output the target (output) sequence by generating it one item at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_2.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input is **3** circles, and the output is **4** triangles. \n",
    "\n",
    "Indeed, for tranlation for example, the length of the input sequence in language A is not necessarily equal to the length of the output sequence in language B. \"Je suis étudiant\" (3 words) becomes \"I am a student\" (4 words). A seq2seq model is able to take a variable-length sequence as an input, and return a variable-length sequence as an output, using a fixed-sized model by encoding many inputs into one vector, and decoding from one vector into many outputs.\n",
    "\n",
    "In order to two that, the encoder and decoder neural networks are (commonly) **RNNs (Recurrent Neural Networks)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-B\"></a>B. Why RNNs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Long-term dependencies\n",
    "\n",
    "Remember, as we saw with Denis with the Time-series Forecasting notebook, recurrent neural networks depend on the previous state for the current state's computation. \n",
    "\n",
    "Instead of simply prediction $Y = f(x)$ as in feed-forward neural networks, recurrent networks do $Y_1 = f(x_1, f(x_0))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/unrolled_rnn.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that at every point in time, it takes as input its own previous state and the new input at that time step.\n",
    "<div class=\"alert alert-success\">\n",
    "RNNs keep information about their previous state.\n",
    "</div>\n",
    "\n",
    "Each state is a function of the previous state, which is the function of its previous state, and so on. So, state n contain information from all past timesteps. And we need this to predict sequences. Indeed, elements in the sequences and the order of these elements have a strong relationship. For instance, a sentence contains words in a certain order and some of which have a strong influence on others. \n",
    "\n",
    "For example, let's translate using Google Translate \"Je surveille mes actions tous les jours\" in english :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/mt_ex_1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's translate \"A la bourse, je surveille mes actions tous les jours\" :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/mt_ex_2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how \"actions\" becomes \"stocks\" thanks to the memory of the RNN when we add the stock market context at the beginning of the sentence.\n",
    "\n",
    "##### - Variable-length sequence\n",
    "\n",
    "The two RNNS working together frees us from sequence length which makes it ideal for translation between two languages and opens a whole new range of problems which can be solved using such architecture. How a sequence starts and ends ? Example with a german to english translator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/sos-eos.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes as input a sequence starting with the SOS (Start Of Sequence) token and ending with the EOS (End Of Sequence) token. As long as the decoder doesn't predict the EOS token, the size of the output sequence can increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-C\"></a>C. LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've talked about RNNs that keep track of temporal dependencies. In reality, simple RNNs can have difficulty handling long-term dependencies. Imagine a sequence problem that involves predicting a word in a sentence. Example : \"In France, I had a great time and I learnt some of the ______ language.\" The information of the word \"France\" can be found in hidden states but often is already too abstract. \n",
    "\n",
    "Moreover, when learning, in some cases the gradient can be very small and can prevent the network weights from changing value. Traditional activation functions such as the hyperbolic tangent function have gradients in the range (-1, 1). Thus, the backpropagation trough time calculates gradients by multiplying very small numbers. And the more we calculate the errors due to further and further back time steps, the smaller and smaller the gradients will be, because of the product of small numbers. Parameters of the simple RNN become biased to capture short-term dependencies. This is called **the Vanishing gradient problem**.\n",
    "\n",
    "A seq2seq model could use more complex recurrent neural networks without this problem, such as **LSTM (Long Short-Term Memory)** or a **GRU (Gated Recurrent Unit)**. \n",
    "\n",
    "##### - LSTM\n",
    "As we saw with Denis with the Time-series Forecasting notebook, a LSTM is an more complex RNN explicitly designed to avoid the long-term dependency problem. Instead of just taking in a hidden state and returning a new hidden state per time-step, a LSTM cell take in and return a separate cell state, $c_t$, per time-step.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    " $h_t = \\text{simpleRNN}(x_t, h_{t-1})\\\\\n",
    " (h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))$\n",
    "</div>\n",
    "Example of a LSTM cell :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/lstm.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1}+b_i)\\\\\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1}+b_f)\\\\\n",
    "c_t = f_tc_{t-1}+i_t\\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\\\\\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_{t-1}+b_o)\\\\\n",
    "h_t = o_t\\tanh(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to LSTMs is the cell state, which, in theory, can carry relevant information trhoughout the entire processing of the sequence. The horizontal line running through the cell is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/cell-state-going-through.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM, using gates, have the ability to remove or add information to the cell state. Gates are a way to optionally let information through and they can be trained to keep information from long ago. It can keep information in the cell state, which might not be relevant at this time step, but might be relevant at a much later time step. Differents types of LSTM cells exist, if you want to learn more about LSTM, you can read [this blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "<div class=\"alert alert-success\">LSTM cells are able to keep track of information throughout many timesteps.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - GRU\n",
    "A GRU, Gated Recurrent Unit, get rid of the cell state, has only two gates (a reset gate and an update gate) and used only the hidden state to transfer information. GRUs are a little speedier to train than LSTMs. Example of a GRU :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/gru.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z_t = \\sigma(W_{xz}x_t + W_{hz}h_{t-1} +b_z)\\\\\n",
    "r_t = \\sigma(W_{xr}x_t + W_{hr}h_{t-1} +b_r)\\\\\n",
    "h_t = z_t h_{t-1} + (1-z_t)\\tanh(W_{xh}x_t + W_{hh}h_{t-1}r_t + b_h)\\\\\n",
    "$$\n",
    "\n",
    "As with LSTMs, there are possible variants of GRU. If you are interested, GRU is the subject of one of AML's notebooks this year (topic number 31), so you can learn more about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-D\"></a>D. Recap\n",
    "\n",
    "Here's a animation showing what we have for a seq2seq machine translation model for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_4.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, the context vector (the final hidden state of the encoder) will contain semantic information about the query sentence that is input to the bot. \n",
    "**Problem**: The context vector is responsible for representing the entire input sequence. The output sequence relies heavily on this vector, making it challenging for the model to deal with long sentences. A solution was proposed in the papers [Bahdanau et al., 2014](https://arxiv.org/pdf/1409.0473.pdf) and [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf) : **attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sec2-E\"></a>E. Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention allows the model to focus on the relevant parts of the input sequence at every stage of the output sequence allowing the context to be preserved from beginning to end. Instead of sending only one single hidden state vector to the decoder, we send a attention vector created from a linear combination of all previous hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_5.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every step the decoder can select a different part of the input sentence to consider. So, every step, a new attention vector (so a new linear combination) is calculated to be relevant for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"852\" height=\"480\" controls src=\"Images/seq2seq_6.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the translation of the sentence \"L'accord sur la zone économique européènne a été signé en août 1992\", we can visualize the attention matrix giving different weights to the hidden states depending on the time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"480\" height=\"360\"  src=\"Images/attention.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the model, for unambiguous words like \"août\" or \"1992\", gives little importance to the other words in the sentence. You can also see that for \"zone économique européenne\", the model adapts to the reversed order between French and English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a nutshell**, we have seen that a seq2seq model is composed of two RNNs working together, one as an encoder and one as a decoder. These RNNs can be simple, or more complex using LSTM or GRU cells. For the context vector passing from the encoder to the decoder, it consists of all the hidden states of the encoder. For each step, an attention vector is computed to give different importance to the different hidden states, so that the decoder can focus on the relevant parts for the time step under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id=\"sec3\"></a>Let's do our translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used is  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import re, unicodedata\n",
    "import random\n",
    "\n",
    "#setting up device for use with pyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device used is \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order not to overload the notebook, a separate file contains the functions for loading and preprocessing the data. You can take a look at it later if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "3273 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      " - fra 1851 words\n",
      " - eng 1030 words\n",
      "Example of a pair :  ['nous sommes en securite.', 'we re safe.']\n"
     ]
    }
   ],
   "source": [
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 3273 pairs of sentences, one in French and another one being its English translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #Embedding Layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        #GRU Layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Embedding the input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/encoder-mt.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer  is used to represent words by dense vectors where a vector represents the projection of the word into a continuous vector space. Each word in the vocabulary corresponds to a single vector of real numbers of fixed dimension. As we have seen in NLP, this is an improvement over the traditional bag-of-word model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #Embedding Layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        #GRU layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        #Linear layer mapping to the output size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Embedding the input and applying relu\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        # We feed the embedded vector as well as the context vector passed as argument into the gru\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/decoder-mt.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p #dropout proba\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #Embedding layers for the decoder input\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #First, the input (target sequence) is embedded. Some weights are randomly zeroed out to facilitate\n",
    "        #learning with the attention mechanism\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        #Attention is computed by combining the context vectors and the embedded input\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        #Attention is applied on the encoded original sentence (in english)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        #We retrieve the embedded input and the context vector (with attention applied), and combine the two tensors\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #The attended part of the input is fed into the lstm, conditioned by the hidden and cell states\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        #Retrieve token probabilities\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        #In addition to the output and hidden states that are necessary for iterating, we return the attention\n",
    "        #weights, that will provide some form of explainability\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/attention-mt.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"BMM\" performs a batch matrix-matrix product of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Functions to Vectorize the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, for each pair we will need an input tensor (index of words in the input sentence) and a target tensor (index of words in the target sentence). When creating these vectors, we will add the EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. One step training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, \n",
    "                                                                decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3. Multiple training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"iter = {0} / {1} = {2:.1f}%, loss avg = {3:.2f}\".format(iter, n_iters, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 100 / 15000 = 0.7%, loss avg = 4.07\n",
      "iter = 200 / 15000 = 1.3%, loss avg = 2.92\n",
      "iter = 300 / 15000 = 2.0%, loss avg = 2.87\n",
      "iter = 400 / 15000 = 2.7%, loss avg = 2.65\n",
      "iter = 500 / 15000 = 3.3%, loss avg = 2.49\n",
      "iter = 600 / 15000 = 4.0%, loss avg = 2.48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3e6c768eca29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-17c0079c3e0c>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 18\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e5d7d3d72446>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# WATCH OUT ! Set the n_iter parameter, try a small value if you do not have a gpu\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "n_iter = 15000 # 15000 iters takes 3 minutes on my computer with a gpu\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv - Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> tu t approches.\n",
      "= you re getting closer.\n",
      "< you re getting closer. <EOS>\n",
      "\n",
      "> je suis vraiment heureuse.\n",
      "= i m really happy.\n",
      "< i m really happy. <EOS>\n",
      "\n",
      "> vous etes tres raffinees.\n",
      "= you re very sophisticated.\n",
      "< you re very sophisticated. <EOS>\n",
      "\n",
      "> tu es geniale.\n",
      "= you re awesome.\n",
      "< you re awesome. <EOS>\n",
      "\n",
      "> vous etes le meilleur.\n",
      "= you re the greatest.\n",
      "< you re the greatest. <EOS>\n",
      "\n",
      "> vous etes sauf.\n",
      "= you re safe.\n",
      "< you re resilient. <EOS>\n",
      "\n",
      "> ce sont des etudiantes.\n",
      "= they re students.\n",
      "< they re students. <EOS>\n",
      "\n",
      "> tu es tatillon.\n",
      "= you re finicky.\n",
      "< you re powerful. <EOS>\n",
      "\n",
      "> nous sommes sensibles.\n",
      "= we re sensitive.\n",
      "< we re contented. <EOS>\n",
      "\n",
      "> tu ne regardes pas.\n",
      "= you aren t looking.\n",
      "< you aren t looking. <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vizualizing attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elles sont mes amies.\")\n",
    "evaluateAndShowAttention(\"nous devenons plus intimes.\")\n",
    "evaluateAndShowAttention(\"vous etes riche.\")\n",
    "evaluateAndShowAttention(\"tu es tres sympa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sources\"></a>Sources\n",
    "\n",
    "Sequence-to-sequence model :\n",
    "- Sequence to Sequence Learning with Neural Networks : [Sutskever et al., 2014](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "- Learning Phrase Representations using RNN Encoder–Decoderfor Statistical Machine Translation : [Cho et al., 2014](https://arxiv.org/pdf/1406.1078v3.pdf)\n",
    "- Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) : [Jay Alammar, 2018](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "- MIT 6.S191 (2018): Sequence Modeling with Neural Networks : [Harini Suresh's Lecture (Youtube video)](https://www.youtube.com/watch?v=CznICCPa63Q)\n",
    "- Attention — Seq2Seq Models : [Pranay Dugar, 2019](https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263)\n",
    "- Recurrent neural network : [Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "- Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText : [Ben Trevett (Github)](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "- Time-series Forecasting : [Denis Wilson's notebook](https://github.com/erachelson/MLclass/tree/master/13%20-%20Time-series%20Forecasting)\n",
    "- Natural Language Processing : [Gautier Durantin's notebooks](https://github.com/erachelson/MLclass/tree/master/12%20-%20Natural%20Language%20Processing)\n",
    "\n",
    "\n",
    "Vanishing grandient problem :\n",
    "- Vanishing gradient problem : [Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "- The Vanishing Gradient Problem : [Chi-Feng Wang, 2019](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "\n",
    "\n",
    "LSTM & GRU :\n",
    "- Long short-term memory : [Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "- Gated recurrent unit : [Wikipedia](https://en.wikipedia.org/wiki/Gated_recurrent_unit)\n",
    "- Illustrated Guide to LSTM’s and GRU’s: A step by step explanation : [Michael Nguyen, 2018](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "- Understanding LSTM Networks : [Christopher Olah, 2015](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Understanding GRU Networks : [Simeon Kostadinov, 2017](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)\n",
    "- Modern Recurrent Neural Networks : [Dive into Deep Learning course](https://d2l.ai/chapter_recurrent-modern/index.html)\n",
    "\n",
    "Data preprocessing :\n",
    "- NLP | Sequence to Sequence Networks| Part 1| Processing text data : [Mohammed Ma'amari, 2018](https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72)\n",
    "\n",
    "Translator code : \n",
    "- [](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
